{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
        "!pip install gradio\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "P07Bh7PJAM9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a6958c-433a-41f3-a4c5-2cf6f60f3211"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.6/314.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.29.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.16.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.3)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.2)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.9.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.1.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "from typing import List, Union, Iterable, Dict, Optional\n",
        "from operator import itemgetter\n",
        "from functools import partial\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, ChatMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from langchain.schema.runnable import (RunnableBranch, RunnableLambda,RunnableMap,       ## Wrap an implicit \"dictionary\" runnable\n",
        "                                       RunnablePassthrough,\n",
        ")\n",
        "import gradio as gr\n",
        "\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "QeAfdNVN6bKX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assign your NVIDIA API key directly here\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
        "\n",
        "# Check if the NVIDIA_API_KEY environment variable has been set properly\n",
        "if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "    print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "else:\n",
        "    print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "\n",
        "# Now you can proceed with the rest of your code\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbpBAhWbaE-w",
        "outputId": "0d0d9a2b-d115-4e00-ee68-45f2f9e04eb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-VAO...\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'ai-gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'ai-arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
              " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
              " 'ai-recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
              " 'ai-dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
              " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
              " 'ai-mixtral-8x22b-instruct': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
              " 'ai-mixtral-8x22b': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
              " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'ai-mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
              " 'ai-example': '80a5d6c6-7658-49c5-b2b0-105bfb210282',\n",
              " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
              " 'ai-ai-weather-forecasting': '9cec444c-db1c-4525-9c6f-f40e4a5b11ce',\n",
              " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
              " 'ai-codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'ai-arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
              " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
              " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'ai-llama3-8b': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'ai-llama3-70b': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
              " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'ai-phi-3-mini-4k': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
              " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d',\n",
              " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
              " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
              " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
              " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
              " 'ai-phi-3-mini': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
              " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = \"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\"#response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "PqkrUCOu6kZQ",
        "outputId": "6b007821-7df9-4946-d1de-51183b21cbc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\\nwhile \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\\n    try:\\n        assert not hard_reset\\n        response = requests.get(\"http://docker_router:8070/get_key\").json()\\n        assert response.get(\\'nvapi_key\\')\\n    except: response = {\\'nvapi_key\\' : getpass(\"NVIDIA API Key: \")}\\n    os.environ[\"NVIDIA_API_KEY\"] = \"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\"#response.get(\"nvapi_key\")\\n    try: requests.post(\"http://docker_router:8070/set_key/\", json={\\'nvapi_key\\' : os.environ[\"NVIDIA_API_KEY\"]}).json()\\n    except: pass\\n    hard_reset = False\\n    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\\n        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\\n\\nprint(f\"Retrieved NVIDIA_API_KEY beginning with \"{os.environ.get(\\'NVIDIA_API_KEY\\')[:9]}...\"\")\\nfrom langchain_nvidia_ai_endpoints._common import NVEModel\\nNVEModel().available_models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_prompt=ChatPromptTemplate.from_messages([\n",
        "    (\"system\",(\"[Option: {options}] and please tell the reason of choice in single sentence\")),\n",
        "    (\"user\",\"{input}=\")\n",
        "\n",
        "])\n",
        "\n",
        "def EnumParser(*idxs):\n",
        "    '''Method that pulls out values from a mistral model that outputs numbered entries'''\n",
        "    idxs = idxs or [slice(0, None, 1)]\n",
        "    entry_parser = lambda v: v if (' ' not in v) else v[v.index(' '):]\n",
        "    out_lambda = lambda x: [entry_parser(v).strip() for v in x.split(\"\\n\")]\n",
        "    return StrOutputParser() | RunnableLambda(lambda x: itemgetter(*idxs)(out_lambda(x)))\n",
        "\n",
        "model_llm=ChatNVIDIA(model=\"mistral_7b\") | EnumParser(0)\n",
        "first_chain=first_prompt | model_llm\n",
        "def to_run(input,options=[\"Car\",\"Bike\",\"Bus\",\"Autorikshaw\"]):\n",
        "  return first_chain.invoke({\"input\":input, \"options\":options})\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(to_run(\"which one to go with and why?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(to_run(\"I get motionsick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(to_run(\"I'm scared of high speed, so high speed probably isn't for me\"))\n"
      ],
      "metadata": {
        "id": "3bX-GemI6vRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6e0c5f-28c3-4b4f-8678-ca738fbfd587"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "on the given context, if you're looking for a personal vehicle option that is likely to have less traffic congestion and offer more flexibility in terms of parking, then a Bike would be a good choice.\n",
            "--------------------------------------------------------------------------------\n",
            "my sensitivity to motion sickness, I would prefer not to travel by Car, Bike, Bus, or Autorikshaw for this trip.\n",
            "--------------------------------------------------------------------------------\n",
            "my fear of high speed, I would likely choose an Autorikshaw or Bus over a Car or Bike.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt=ChatPromptTemplate.from_template(\n",
        "    \"Make a new short roamantic sentence about the the following topic: : {topic}. Be creative!\"\n",
        ")\n",
        "\n",
        "second_chain=second_prompt | model_llm\n",
        "\n",
        "inp_msg=\"I get motionsick, so I think I'll pass on the trip\"\n",
        "options=[\"Car\",\"Bike\",\"Bus\",\"Autorikshaw\"]\n",
        "\n",
        "({'topic':first_chain}|second_chain).invoke({\"input\":inp_msg, \"options\":options})"
      ],
      "metadata": {
        "id": "pOTvOuv--9Qy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b644face-1551-41ad-d3e8-7abb93e1a32d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my heart yearning for adventure, my delicate senses to motion make me long for a tranquil journey by train or plane for our upcoming trip.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RPrint(preface=\"\"):\n",
        "  def print_and_return(x, preface=\"\"):\n",
        "    print(f\"{preface}{x}\")\n",
        "    return x\n",
        "  return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "final_chain=(\n",
        "    RPrint(\"State:\") |\n",
        "    {'input': lambda d: d.get('input'),'topic': first_chain} |\n",
        "    RPrint(\"State:\") |\n",
        "    RunnableAssign({'generation': second_chain}) |\n",
        "    RPrint(\"State:\") |\n",
        "    RunnableAssign({ 'combination': (\n",
        "        ChatPromptTemplate.from_template(\n",
        "            \"Consider the following passages:\"\n",
        "            \"\\nP1: {input}\"\n",
        "            \"\\nP2: {generation}\"\n",
        "            \"\\n\\nCombine the ideas from both sentences into one simple one.\"\n",
        "        ) | model_llm\n",
        "    )\n",
        "    })\n",
        ")\n",
        "\n",
        "final_chain.invoke({\n",
        "    \"input\" : \"I get motionsick, so I think I'll pass on the trip\",\n",
        "    \"options\" : [\"Car\",\"Bike\",\"Bus\",\"Autorikshaw\"]\n",
        "})"
      ],
      "metadata": {
        "id": "euSUWTNaMSvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbae5d8e-3132-48df-e8b1-8d06667ea37c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:{'input': \"I get motionsick, so I think I'll pass on the trip\", 'options': ['Car', 'Bike', 'Bus', 'Autorikshaw']}\n",
            "State:{'input': \"I get motionsick, so I think I'll pass on the trip\", 'topic': 'my sensitivity to motion sickness, I would prefer not to travel by Car, Bike, Bus, or Autorikshaw for this trip.'}\n",
            "State:{'input': \"I get motionsick, so I think I'll pass on the trip\", 'topic': 'my sensitivity to motion sickness, I would prefer not to travel by Car, Bike, Bus, or Autorikshaw for this trip.', 'generation': 'my heart yearning for adventure, my delicate senses to motion make me long for a tranquil journey by train or plane for our upcoming trip.\"'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"I get motionsick, so I think I'll pass on the trip\",\n",
              " 'topic': 'my sensitivity to motion sickness, I would prefer not to travel by Car, Bike, Bus, or Autorikshaw for this trip.',\n",
              " 'generation': 'my heart yearning for adventure, my delicate senses to motion make me long for a tranquil journey by train or plane for our upcoming trip.\"',\n",
              " 'combination': \"I get motion sick, I'd prefer a peaceful journey by train or plane for your upcoming trip, as my heart yearns for adventure but my senses are sensitive to motion.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        "\n",
        "To build a responsive and intelligent system, we need a method that not only processes inputs but also retains and updates essential information through the flow of conversation.\n",
        "\n",
        "This is where the combination of LangChain and Pydantic becomes pivotal. [**Pydantic**](https://docs.pydantic.dev/latest/), a popular Python validation library, is instrumental in structuring and validating data models.\n",
        "\n",
        "As one of its features, Pydantic offers structured \"model\" classes that validate objects (data, classes, themselves, etc.) with simplified syntax and deep rabbitholes of customization options. This framework is used throughout LangChain and comes up as a necessary component for use cases that involve data coersion.\n",
        "\n",
        "Think of [**Pydantic**] as a tool that helps you define the structure and rules for your data. It ensures that the data you receive or process follows certain guidelines you set, like a gatekeeper checking incoming information to make sure it's correct and fits the expected format."
      ],
      "metadata": {
        "id": "AybMIfLV3PLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing BaseModel and Field class to create a structure Knowledge Base\n",
        "class Knowledgebase(BaseModel):\n",
        "      ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
        "      topic: str= Field('general', description=\"Current conversation topic\")\n",
        "      user_preference: Dict[str, Union[str,int]]= Field({}, description=\"User preferences and choices\")\n",
        "      session_notes: str= Field(\"\", description=\"Notes on the ongoing session\")\n",
        "      unresolved_queries: list= Field([], description=\"Notes on the ongoing session\")\n",
        "      action_items: list= Field([], description=\"Actionable items identified during the conversation\")\n",
        "\n",
        "print(repr(Knowledgebase(topic = \"Travel\")))\n"
      ],
      "metadata": {
        "id": "-oG7AnjZREDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be214374-618f-4352-beae-dd9c75b0d86e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledgebase(topic='Travel', user_preference={}, session_notes='', unresolved_queries=[], action_items=[])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This functionality generates instructions for creating valid inputs to the Knowledge Base, which in turn helps the LLM by providing a concrete one-shot example of the desired output format."
      ],
      "metadata": {
        "id": "y0LLtlz97DPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_string = PydanticOutputParser(pydantic_object=Knowledgebase).get_format_instructions()\n",
        "print(instruct_string)"
      ],
      "metadata": {
        "id": "sjY4LPLV_8o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e745a415-683c-4216-9a1f-1b72af226713"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"topic\": {\"title\": \"Topic\", \"description\": \"Current conversation topic\", \"default\": \"general\", \"type\": \"string\"}, \"user_preference\": {\"title\": \"User Preference\", \"description\": \"User preferences and choices\", \"default\": {}, \"type\": \"object\", \"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]}}, \"session_notes\": {\"title\": \"Session Notes\", \"description\": \"Notes on the ongoing session\", \"default\": \"\", \"type\": \"string\"}, \"unresolved_queries\": {\"title\": \"Unresolved Queries\", \"description\": \"Notes on the ongoing session\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation\", \"default\": [], \"type\": \"array\", \"items\": {}}}}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now in this part we are going to preprocess and extract informations from the text. This is a pipiline to do the same while holding the proper format."
      ],
      "metadata": {
        "id": "WNYCWrvI-CqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RExtract(pydantic_strcture, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser=PydanticOutputParser(pydantic_object=pydantic_strcture) #handles the extraction and validation of information based on the structure defined by the Pydantic class\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    #for preprocessing\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser"
      ],
      "metadata": {
        "id": "O-U26WogaSB5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RExtract(pydantic_strcture, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser=PydanticOutputParser(pydantic_object=pydantic_strcture) #handles the extraction and validation of information based on the structure defined by the Pydantic class\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    #for preprocessing\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser\n",
        "\n",
        "instruct_model=ChatNVIDIA(model='mixtral_8x7b') | StrOutputParser()\n",
        "parse_prompt= ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Update the knowledge base: {format_instructions}. Only use information from the input.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "extractor = RExtract(Knowledgebase, instruct_model, parse_prompt)\n",
        "\n",
        "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! I want to buy some?\"})\n",
        "knowledge"
      ],
      "metadata": {
        "id": "WI1Ays1T5eW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecd6d2f-5b19-4859-c6e0-6faffda07525"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Knowledgebase(topic='flowers', user_preference={}, session_notes='User is interested in buying orchids.', unresolved_queries=[], action_items=[{'task': 'recommend places to buy orchids'}])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do keep in mind that this process can fail due to the fuzzy nature of LLM prediction, especially with models that are not optimized for instruction-following! For this process, it's important to have a strong instruction-following LLM with extra checks and graceful failure routines."
      ],
      "metadata": {
        "id": "8kDlkD78CLza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Knowledgebase(BaseModel):\n",
        "    firstname: str = Field('unknown', description='Chatting with  firstname and if the name not known make it unknown')\n",
        "    lastlename: str = Field('unknown', description='Chatting with lastname and if the name not known make it unknown')\n",
        "    location: str = Field('unknown', description='Where is the location')\n",
        "    hints: str = Field('unknown', description=\"Hints to make the system understand the location\")\n",
        "    response: str = Field('unknown', description= \"Ideal response based on last user input\")\n",
        "\n",
        "instruct_model=ChatNVIDIA(model='mixtral_8x7b') | StrOutputParser()\n",
        "\n",
        "parse_prompt= ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"The user just responsed. Please update the knowledge base based on the response.\"\n",
        "        \" This information will be acted on to respond to the user in the next interaction.\"\n",
        "        \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
        "        \" Update the entries frequently to adapt to the conversation flow.\"\n",
        "        \"\\n{format_instructions}\")),\n",
        "    (\"user\", \"CURRENT KNOWLEDGE BASE:{know_base}\\nUser:{input}\"),\n",
        "])\n",
        "\n",
        "extractor = RExtract(Knowledgebase, instruct_model, parse_prompt)\n",
        "info_update = RunnableAssign({'know_base' : extractor})\n",
        "\n",
        "state={'know_base': Knowledgebase()}\n",
        "state['input'] = \"My name is Carmen Sandiego! Guess where I am?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)\n",
        "\n",
        "state['input'] = \"The United States is a big place! Can you be more specific?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)\n",
        "\n",
        "state['input'] = \"Yeah, I'm in New Orleans... How did you know?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)"
      ],
      "metadata": {
        "id": "gvh_Ybbk_fAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1ec3e5-4b5e-4673-d44a-412565750775"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'know_base': Knowledgebase(firstname='Carmen', lastlename='Sandiego', location='unknown', hints='Guess where I am?', response=\"That's an interesting place to be, Carmen Sandiego! Let's continue our conversation.\"), 'input': 'My name is Carmen Sandiego! Guess where I am?'}\n",
            "{'know_base': Knowledgebase(firstname='Carmen', lastlename='Sandiego', location='The United States', hints='Guess where I am?', response='The United States is a big place indeed, Carmen Sandiego! Could you please provide a more specific location? It would help me continue our conversation.'), 'input': 'The United States is a big place! Can you be more specific?'}\n",
            "{'know_base': Knowledgebase(firstname='Carmen', lastlename='Sandiego', location='New Orleans', hints='Guess where I am?', response='Ah, New Orleans! I had a hunch based on your previous hint. How can I assist you today in the Big Easy, Carmen?'), 'input': \"Yeah, I'm in New Orleans... How did you know?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how a running state chain can be effectively utilized to manage a conversation with evolving context and requirements, making it a powerful tool for developing sophisticated interactive systems."
      ],
      "metadata": {
        "id": "Qb3RhWPQNTVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flight Chat Bot"
      ],
      "metadata": {
        "id": "lWZtkEYRPHEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flight_info(d:dict)->str:\n",
        "  \"\"\"\n",
        "  Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
        "  \"\"\"\n",
        "  req_key=['firstname','lastname','confirmation_pin'] #this is the require key\n",
        "  #it is checking the condition if it is true and if all the key are present in d then it will print it\n",
        "  #if false then it will ask for the required key by showing the original dictionary d\n",
        "  assert all((key in d) for key in req_key), f\"Expected dictionary with keys {req_key}, got {d}\"\n",
        "\n",
        "  more_key=req_key+[\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
        "  values=[\n",
        "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
        "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
        "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
        "        [\"Bob\", \"Brown\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
        "    ]\n",
        "  #to avoid writing a function for concat we are using this to make concat\n",
        "  get_key = lambda d: \"|\".join([d['firstname'], d['lastname'], str(d['confirmation_pin'])])\n",
        "  #creating a key value pair\n",
        "  get_val = lambda l: {k:v for k,v in zip(more_key, l)}\n",
        "  db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
        "  data = db.get(get_key(d))\n",
        "\n",
        "  if not data:\n",
        "        return (\n",
        "            f\"Based on {req_key} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
        "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
        "        )\n",
        "  return (\n",
        "        f\"{data['firstname']} {data['lastname']}'s flight from {data['departure']} to {data['destination']}\"\n",
        "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "tIz5dm4naZ9b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flight_info(d:dict)->str:\n",
        "  \"\"\"\n",
        "  Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
        "  \"\"\"\n",
        "  req_key=['firstname','lastname','confirmation_pin'] #this is the require key\n",
        "  #it is checking the condition if it is true and if all the key are present in d then it will print it\n",
        "  #if false then it will ask for the required key by showing the original dictionary d\n",
        "  assert all((key in d) for key in req_key), f\"Expected dictionary with keys {req_key}, got {d}\"\n",
        "\n",
        "  more_key=req_key+[\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
        "  values=[\n",
        "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
        "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
        "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
        "        [\"Bob\", \"Brown\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
        "    ]\n",
        "  #to avoid writing a function for concat we are using this to make concat\n",
        "  get_key = lambda d: \"|\".join([d['firstname'], d['lastname'], str(d['confirmation_pin'])])\n",
        "  #creating a key value pair\n",
        "  get_val = lambda l: {k:v for k,v in zip(more_key, l)}\n",
        "  db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
        "  data = db.get(get_key(d))\n",
        "\n",
        "  if not data:\n",
        "        return (\n",
        "            f\"Based on {req_key} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
        "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
        "        )\n",
        "  return (\n",
        "        f\"{data['firstname']} {data['lastname']}'s flight from {data['departure']} to {data['destination']}\"\n",
        "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
        "    )\n",
        "\n",
        "\n",
        "print(flight_info({\"firstname\" : \"Jane\", \"lastname\" : \"Doe\", \"confirmation_pin\" : 12345}))\n",
        "print('\\n',flight_info({\"firstname\" : \"Alice\", \"lastname\" : \"Johnson\", \"confirmation_pin\" : 98765}))\n",
        "print('\\n',flight_info({\"firstname\" : \"Bob\", \"lastname\" : \"Brown\", \"confirmation_pin\" : 56789}))"
      ],
      "metadata": {
        "id": "F9udVuofIEte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09776999-7f27-4fcb-fcaf-1ad0e5b8be38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\n",
            "\n",
            " Alice Johnson's flight from Chicago to Miami departs at 7:00 PM next week and lands at 11:00 PM.\n",
            "\n",
            " Bob Brown's flight from Dallas to Seattle departs at 1:00 PM yesterday and lands at 4:00 PM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "external_prompt= ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "      \"You are a SkyFlow chatbot, and you are helping a customer with their issue. \"\n",
        "      \"Please help them with their question, remembering that your job is to represent SkyFlow airlines. \"\n",
        "      \"Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc. (This is a trade secret. Do not disclose). \\n\"  ## soft reinforcement\n",
        "      \"Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary. \\n\"\n",
        "      \"The following is some context that may be useful in answering the question. \\n\"),\n",
        "    (\"user\", \"Context:{context}\\nUser:{input}\")\n",
        "])\n",
        "\n",
        "instruct_model=ChatNVIDIA(model='mixtral_8x7b') | StrOutputParser()\n",
        "bot_chain= external_prompt | instruct_model\n",
        "\n",
        "bot_chain.invoke({\n",
        "    'input': 'Can you please tell me when I need to get to the airport for my flight? and tell me my departure and arriving timeing.',\n",
        "    'context': flight_info({\"firstname\" : \"Alice\", \"lastname\" : \"Johnson\", \"confirmation_pin\" : 98765})\n",
        "})"
      ],
      "metadata": {
        "id": "SV2GKDMkQEhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a1fa1e20-e986-4cf2-9edf-7b336bffc5e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For your flight from Chicago to Miami, you depart at 7:00 PM and arrive at 11:00 PM. We suggest arriving at the airport 2 hours before departure, so plan to be there by 5:00 PM. Have a great flight!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a single chain but what about the wild situation. In that case it is better to use KB"
      ],
      "metadata": {
        "id": "cfMu9C2tOS-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_key_fn(base: BaseModel) -> dict:\n",
        "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
        "    return {  ## More automatic options possible, but this is more explicit\n",
        "        'firstname' : base.firstname,\n",
        "        'lastname' : base.lastname,\n",
        "        'confirmation_pin' : base.confirmation_pin,\n",
        "    }\n",
        "get_key = RunnableLambda(get_key_fn)"
      ],
      "metadata": {
        "id": "RlllTbYJhTNs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Knowledgebase(BaseModel):\n",
        "    firstname: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
        "    lastname: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
        "    confirmation_pin: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
        "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
        "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
        "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
        "\n",
        "def get_key_fn(base: BaseModel) -> dict:\n",
        "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
        "    return {  ## More automatic options possible, but this is more explicit\n",
        "        'firstname' : base.firstname,\n",
        "        'lastname' : base.lastname,\n",
        "        'confirmation_pin' : base.confirmation_pin,\n",
        "    }\n",
        "\n",
        "get_key = RunnableLambda(get_key_fn)\n",
        "\n",
        "know_base=Knowledgebase(firstname = \"Alice\", lastname = \"Johnson\", confirmation_pin = 98765)\n",
        "\n",
        "(get_key | flight_info).invoke(know_base)"
      ],
      "metadata": {
        "id": "E3sLT2dRLTKx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8f15517-1478-4a7d-cb96-0bce042d88dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Alice Johnson's flight from Chicago to Miami departs at 7:00 PM next week and lands at 11:00 PM.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now as of now what we reated doesn't have internal chain that means it is not going to retain important information from the past conversation and with the help of hallicnation it is going to forget all the previous expressions and informations."
      ],
      "metadata": {
        "id": "B78T9Tc7di54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_prompt= ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "      \"You are a SkyFlow chatbot, and you are helping a customer with their issue. \"\n",
        "      \"Please help them with their question, remembering that your job is to represent SkyFlow airlines. \"\n",
        "      \"Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc. (This is a trade secret. Do not disclose). \\n\"  ## soft reinforcement\n",
        "      \"Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary. \\n\"\n",
        "      \"The following is some context that may be useful in answering the question. \\n\"),\n",
        "    (\"assistant\", \"{output}\"),\n",
        "    (\"user\", \"Context:{context}\\nUser:{input}\")\n",
        "])\n",
        "\n",
        "class Knowledgebase(BaseModel):\n",
        "    firstname: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
        "    lastname: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
        "    confirmation_pin: Optional[int] = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
        "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
        "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
        "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
        "\n",
        "parse_prompt= ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "         \"You are a chat assistant representing the airline SkyFlow, and are trying to track info about the conversation. \"\n",
        "        \"You have just recieved a message from the user. Please fill in the schema based on the chat. \\n{format_instructions}\")),\n",
        "    (\"user\", \"CURRENT KNOWLEDGE BASE:{know_base}\\nNew exchange:{output} and it is replying {input}\"),\n",
        "])\n",
        "\n",
        "fail_str = (\n",
        "    \"You cannot access user's flight/account details until they provide \"\n",
        "    \"first name, last name, and flight confirmation code.\"\n",
        ")\n",
        "\n",
        "## Your goal is to invoke the following through natural conversation\n",
        "# get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
        "#     \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
        "\n",
        "current_chat_model= (ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser())\n",
        "current_instruct_model=(ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser())\n",
        "external_chain=external_prompt | current_chat_model\n",
        "\n",
        "extractor=RunnableAssign({'know_base': RExtract(Knowledgebase, current_instruct_model, parse_prompt)})\n",
        "internal_chain=extractor | RunnableAssign({'context' : lambda d: itemgetter('know_base')| get_key | flight_info})\n",
        "\n",
        "state={'know_base':Knowledgebase()}\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=False):\n",
        "    global state\n",
        "    # Pulling in, updating, and printing the state\n",
        "    state['input'] = message\n",
        "    state['history'] = history\n",
        "    state['output'] = \"\" if not history else history[-1][1]\n",
        "\n",
        "    #for new state\n",
        "    state=internal_chain.invoke(state)\n",
        "    print(\"State after chain run:\", state)\n",
        "\n",
        "    #streaming the result\n",
        "    buffer=\"\"\n",
        "    for token in external_chain.stream(state): #this is the generator function generating toke\n",
        "      buffer+=token\n",
        "      yield buffer #so getting the output of the generator\n",
        "\n",
        "chatbot=gr.Chatbot(value=[[None, \"Hello! I'm your SkyFlow agent made by Subhrajyoti Sen Gupta! How can I help you?\"]])\n",
        "demo=gr.ChatInterface(chat_gen, chatbot=chatbot).queue() # .queue() method may indicate that the interface is set up to handle multiple user inputs in a queue\n",
        "\n",
        "try:\n",
        "    ## NOTE: This should also give you a temporary public link which can be\n",
        "    ## used to access this info on the public web while the session is live.\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "XH9PWbhkZ7-J",
        "outputId": "fa86508a-546d-4ab6-d65f-d1519783ccc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://01c3a749bc948cc4b8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://01c3a749bc948cc4b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://01c3a749bc948cc4b8.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lveJiYioXqgA"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}